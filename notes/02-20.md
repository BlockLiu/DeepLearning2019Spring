## 数据

### Data bugs

- improper input：

- augmentation：random cropping

  - 希望神经网络的训练数据是有一定难度的
  - 例：把图片输入增广成多个图片输入
  - Rule-of-thumb：

- what the classifier really is：

  - solution：multi-task learning，即不再只是简单的给一个分类结果，还可以附加一些别的结果

- sample with replacement is bad：

  - np.random.choice默认是带放回的抽样

- evaluation metric can be wrong

### Data checklist

- training data：
  - 检查数据
    - 任务是否可以完成（人工先做一下）
    - 数据去重&shuffle
  - 检查增广数据
- test data：
  - 是否适合training data来自同一个分布（同源）？
  - test data是否和training data真的独立？
    - 比如，taining data和test data中的人脸图片都来自同一个人
  - 去重

------



## Image Classification, Linear Classifier, Loss Function and Optimization

### Image Classification 任务

- 图片 & 离散的label
- challanges：
  - semantic gap：人可以识别图片，但是图片实际上只是一个像素table
  - viewpoint variation：视角变化，图片变化
  - illumination：光照
  - Deformation：某些物体会形变（比如猫。。。）
- 机器学习：data-driven approach
  - 通过手机各种各样情况下的数据来解决上述的问题

### First classifier：Nearst Neighbor

- 定义图片之间的距离：
  - 最简单的：L1-distance
- problems：
  - 如何从N个example中找到最近的neighbor？
  - 如何减少噪声的影响？
    - k-nearest neighbors，k越大，对noise的容忍性越好，但是也会造成可划分区域的缩小，降低泛化能力
- k-nearest neighbors的问题：
  - 超参数：
    - k设置为多大？
    - 使用什么方式计算距离
- 使用交叉验证：
  - train、validation（选超参数）、test（evaluate）
- 然而KNN在image classification中从不使用（方法简单，但是在数学上其实很复杂）
  - predict速度太慢
  - 像素之间的距离难以计算
  - 维度灾难：向量的维数过高，使得数据密度降低
### Linear Classification
- $f(x, W) = Wx + b$
- 代数角度：线性变化
- 视觉角度：
- 如何利用数据确定W和b？ -- score function
- W不是唯一的，因为W的线性增长都可以

  - 使用正则化来决定一个W
    - 简单的例子：L2-norm, L1-norm, Elastic net（L1+L2）
      - 需要了解各个regularization term会让W有什么样的趋势？比如L2-norm倾向于让W均匀散开，而L1-norm倾向于让W稀疏
    - 复杂一点的：Dropout、Batch normalization、Stochastic depth、fractional pooling等等

### SoftMax Classifier：Multinomial Logistic Regression

- 将linear function的值变为概率分布
- $s = f(x_i;W),~~~~~~P(Y=k|X=x_i)=\frac{e_sk}{\sum_j e^sj},~~~~~~L_i = -log~P(Y=k|X=x_i)$
- Loss function就定义为：
  - 利用概率分布之间的距离：Kullback-Leibler divergence,  $D_{KL}(P||Q) = \sum_y P(y) log \frac{P(y)}{Q(y)}​$
  - Cross Entropy   $H(P,Q) = \sum_y P(y)\cdot log(\frac{1}{Q(y)})$

### Optimization：快速降低loss

- 梯度下降：计算loss function关于W的梯度
  - 随机梯度下降
### Linear classification is insufficient：需要特征提取
- 很多输入是线性不可分的
  - 可以通过合理的离散化编码将输入变得线性可分

- 使用pixels作为图像的特征输入是不好的
  - 需要特征提取
  - 例：Histogram of Oriented Gradients（HoG），这个特征提取关注边缘特征
           Bag of Words