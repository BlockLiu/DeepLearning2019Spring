## Part1: Neural Network Basics

### Motivation

- feature extrctor is non-linear：

  - shallow：使用线性组合（linear combination）的方法得到non-linear extractor：$f(x) \approx \sum_j g_j$
    - 例如：kernel learning、Boosting
    - 缺陷：需要的template（$g_j$）很多
    - 本质是枚举
  - deep：使用函数复合（composition）：$f(x) \approx g_1(g_2(\ldots g_n(x) \ldots ))$
    - more efficient than linear combination
    - 缺陷：难以优化（非凸、非线性）

- 为了优化composition的non-linear system，使用end-to-end learning

### How to Build Deep Network

- non-linear layers （neuron）design：

  - Linear Projection + Non-linear Activation
### Gradient-based training

  - forward propagation：逐层计算output和loss

      - 作用：计算预测值
      - loss function：评估预测值和真实值之间的差距

  - backward propagation：

      - 作用：基于loss调整参数（计算loss关于parameters的导数）

  - update parameters（optimization）：

      - stochastic gradient descent（on mini-batches）
          - $\theta \leftarrow \theta - \eta \frac{\partial L}{\partial \theta}​$
      - stochastic gradient descent with momentum：
          - $v_t = \gamma \cdot v_{t-1}+\eta \frac{\partial L}{\partial \theta}$
          - $\theta \leftarrow \theta - v_t $
### CNNs

- Fulli-connected Net：每个神经元连接图片所有pixel的组合
- Local-connected Net：不同神经元连接图片不同的pixel的组合
- Convolutional Net：图像平移不变（可节省参数量）
  - multiple filters：每个filter检测图片的一个特征（filter参数个数即一个扫描窗口的大小）
  - 参数个数：filter数量*每个filter的参数个数x1

## Part2: Architecture Design

### 